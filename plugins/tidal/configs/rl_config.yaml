# configs/rl_config.yaml
#
# Configuration for RL Gating Controller training.
# This file extends base_config.yaml with RL-specific settings.

# ============================================================================
# RL AGENT CONFIGURATION
# ============================================================================

# Agent architecture
RL_AGENT_TYPE: "beta"  # Options: "beta" (Beta distribution), "gaussian" (Gaussian + squashing)
RL_OBSERVATION_DIM: 64
RL_ACTION_DIM: 1  # [modulation] on conservative-to-exploratory axis
RL_HIDDEN_DIM: 128

# ============================================================================
# PPO HYPERPARAMETERS
# ============================================================================

# Core PPO settings
RL_LEARNING_RATE: 3.0e-4
RL_GAMMA: 0.99  # Discount factor
RL_GAE_LAMBDA: 0.95  # GAE lambda for advantage estimation
RL_CLIP_EPSILON: 0.2  # PPO clipping parameter
RL_ENTROPY_COEF: 0.01  # Entropy bonus coefficient (initial)
RL_ENTROPY_COEF_FINAL: 0.03  # Entropy bonus coefficient (final, gentle 3x ramp)
RL_VALUE_COEF: 0.5  # Value loss coefficient
RL_MAX_GRAD_NORM: 0.5  # Gradient clipping

# Entropy schedule: "linear" (default) or "homeostasis" (adaptive closed-loop)
RL_ENTROPY_SCHEDULE: "homeostasis"

# Homeostatic entropy regulation (used when RL_ENTROPY_SCHEDULE = "homeostasis")
RL_POLICY_ENTROPY_TARGET: -1.0  # Target policy entropy (Beta dist entropy)
RL_ENTROPY_HOMEOSTASIS_RELEASE_RATE: 0.05  # Boost rate when entropy < target
RL_ENTROPY_HOMEOSTASIS_DECAY_RATE: 0.95  # Decay rate toward baseline
RL_ENTROPY_COEF_MIN: 0.01  # Floor for entropy coefficient
RL_ENTROPY_COEF_MAX: 0.5  # Ceiling for entropy coefficient

# Training loop
RL_ROLLOUT_STEPS: 128  # Steps per rollout before update
RL_NUM_EPOCHS: 4  # PPO epochs per rollout
RL_BATCH_SIZE: 32  # Minibatch size for updates
RL_TOTAL_TIMESTEPS: 100000  # Total training steps
RL_EMA_ALPHA: 0.05  # Smoothing factor for episode stat tracking (replaces deque)

# Beta distribution concentration cap (prevents entropy collapse)
# Beta(15,15) entropy â‰ˆ -0.8; allows peaked distributions for exploitation
RL_BETA_CONCENTRATION_MAX: 15.0

# ============================================================================
# ENVIRONMENT CONFIGURATION
# ============================================================================

RL_MAX_EPISODE_LENGTH: 50  # Max tokens per generation episode
RL_PROMPT_MIN_LENGTH: 3
RL_PROMPT_MAX_LENGTH: 10
RL_TOP_K: 40  # Top-k sampling during generation

# ============================================================================
# GATING MODULATOR SETTINGS
# ============================================================================

# Base generation parameters (before gating modulation)
RL_BASE_TEMPERATURE: 1.0
RL_BASE_REPETITION_PENALTY: 1.2

# Modulation gate effects on temperature
# Result: base_temp * (min + modulation * (max - min))
RL_MODULATION_TEMP_MIN: 0.3  # At modulation=0 (conservative): temp = 0.3x base
RL_MODULATION_TEMP_MAX: 2.0  # At modulation=1 (exploratory): temp = 2.0x base

# Modulation gate effects on nucleus (top-p) sampling
# Result: top_p_min + modulation * (top_p_max - top_p_min)
RL_MODULATION_TOP_P_MIN: 0.7  # At modulation=0: narrow nucleus
RL_MODULATION_TOP_P_MAX: 1.0  # At modulation=1: full distribution

# Modulation gate effects on repetition penalty
# Result: penalty in range [min, max] based on modulation level
RL_MODULATION_PENALTY_MIN: 1.0  # At modulation=0: no extra penalty
RL_MODULATION_PENALTY_MAX: 3.5  # At modulation=1: 3.5x penalty

# Modulation gate effects on top-k filtering
# Result: top_k_min + modulation * (top_k_max - top_k_min)
RL_MODULATION_TOP_K_MIN: 5    # At modulation=0: sharp sampling (only top 5 tokens)
RL_MODULATION_TOP_K_MAX: 100  # At modulation=1: wide sampling (top 100 tokens)

# ============================================================================
# REWARD FUNCTION WEIGHTS
# ============================================================================

# Multi-component reward: reward = sum(weight_i * component_i)
RL_REWARD_PERPLEXITY_WEIGHT: 0.30  # Lower perplexity = better
RL_REWARD_DIVERSITY_WEIGHT: 0.25   # Higher diversity = better (raw logits entropy)
RL_REWARD_SAMPLING_WEIGHT: 0.15    # Sampling entropy of post-filtered distribution
RL_REWARD_REPETITION_WEIGHT: 0.20  # Lower repetition = better
RL_REWARD_COHERENCE_WEIGHT: 0.10   # Higher coherence = better

# Reward normalization
RL_PERPLEXITY_CLIP: 100.0  # Max perplexity before clipping
RL_ENTROPY_TARGET: 5.0  # Target entropy for diversity (raw logits)
RL_SAMPLING_ENTROPY_TARGET: 2.5  # Target entropy for focus (post-filtered, lower than raw)

# ============================================================================
# EVALUATION AND ABLATION
# ============================================================================

RL_EVAL_EPISODES: 50  # Episodes per policy during ablation
RL_EVAL_INTERVAL: 1000  # Steps between evaluations

# ============================================================================
# LOGGING AND CHECKPOINTS
# ============================================================================

RL_LOG_INTERVAL: 10  # Iterations between console logs
RL_METRICS_INTERVAL: 50  # Iterations between saving metrics
RL_CHECKPOINT_INTERVAL: 100  # Iterations between checkpoints

# TensorBoard tags for RL metrics
TENSORBOARD_TAGS:
  LOSSES: "Loss"
  LEARNING_RATE: "Learning"
  RL_REWARDS: "RL/Rewards"
  RL_POLICY: "RL/Policy"
  RL_GATING: "RL/Gating"
