# configs/rl_config.yaml
#
# Configuration for RL Gating Controller training.
# This file extends base_config.yaml with RL-specific settings.

# ============================================================================
# RL AGENT CONFIGURATION
# ============================================================================

# Agent architecture
RL_AGENT_TYPE: "beta"  # Options: "beta" (Beta distribution), "gaussian" (Gaussian + squashing)
RL_OBSERVATION_DIM: 64
RL_ACTION_DIM: 1  # [modulation] on conservative-to-exploratory axis
RL_HIDDEN_DIM: 128

# ============================================================================
# PPO HYPERPARAMETERS
# ============================================================================

# Core PPO settings
RL_LEARNING_RATE: 3.0e-4
RL_GAMMA: 0.99  # Discount factor
RL_GAE_LAMBDA: 0.95  # GAE lambda for advantage estimation
RL_CLIP_EPSILON: 0.2  # PPO clipping parameter
RL_ENTROPY_COEF: 0.01  # Entropy bonus coefficient (initial)
RL_ENTROPY_COEF_FINAL: 0.03  # Entropy bonus coefficient (final, gentle 3x ramp)
RL_VALUE_COEF: 0.5  # Value loss coefficient
RL_MAX_GRAD_NORM: 0.5  # Gradient clipping

# Entropy schedule: "linear" (default) or "homeostasis" (adaptive closed-loop)
RL_ENTROPY_SCHEDULE: "homeostasis"

# Homeostatic entropy regulation (used when RL_ENTROPY_SCHEDULE = "homeostasis")
RL_POLICY_ENTROPY_TARGET: -0.35  # Target policy entropy (1D Beta dist entropy)
RL_ENTROPY_HOMEOSTASIS_RELEASE_RATE: 0.05  # Boost rate when entropy < target
RL_ENTROPY_HOMEOSTASIS_DECAY_RATE: 0.95  # Decay rate toward baseline
RL_ENTROPY_COEF_MIN: 0.01  # Floor for entropy coefficient
RL_ENTROPY_COEF_MAX: 0.5  # Ceiling for entropy coefficient

# Homeostatic diversity regulation (activated by presence of target key)
# Boosts diversity reward weight when mean diversity drops below target,
# then decays back to baseline when diversity is healthy. See ADR 0005.
RL_DIVERSITY_HOMEOSTASIS_TARGET: 0.55  # Target diversity floor (between random 0.55 and fixed 0.72)
RL_DIVERSITY_HOMEOSTASIS_RELEASE_RATE: 0.03  # Boost rate when diversity < target
RL_DIVERSITY_HOMEOSTASIS_DECAY_RATE: 0.95  # Decay rate toward baseline
RL_DIVERSITY_WEIGHT_MIN: 0.15  # Floor for diversity weight (ADR 0003 baseline)
RL_DIVERSITY_WEIGHT_MAX: 0.35  # Ceiling for diversity weight

# ============================================================================
# CONSTRAINED PPO (PPO-LAGRANGIAN)
# ============================================================================
# When RL_CONSTRAINT_MODE is "lagrangian", diversity is enforced as a hard
# constraint instead of a weighted reward component. The diversity and repetition
# weights are zeroed and remaining weights renormalized. A learned Lagrange
# multiplier automatically scales the penalty for constraint violations.
# Overrides DiversityHomeostasis when active.
#
RL_CONSTRAINT_MODE: "weighted"          # Options: "weighted" (default), "lagrangian"
RL_DIVERSITY_CONSTRAINT_THRESHOLD: 0.40  # Lowered from 0.55 — diagnostic: slack constraint
RL_LAGRANGE_MULTIPLIER_LR: 0.05         # Learning rate for dual variable
RL_LAGRANGE_MULTIPLIER_INIT: 1.0        # Initial Lagrange multiplier value

# Training loop
RL_ROLLOUT_STEPS: 128  # Steps per rollout before update
RL_NUM_EPOCHS: 4  # PPO epochs per rollout
RL_BATCH_SIZE: 32  # Minibatch size for updates
RL_TOTAL_TIMESTEPS: 100000  # Total training steps
RL_EMA_ALPHA: 0.05  # Smoothing factor for episode stat tracking (replaces deque)

# Beta distribution concentration cap (prevents entropy collapse)
# Beta(15,15) entropy ≈ -0.8; 1D Beta entropy ranges from ~-0.5 to ~-0.7
RL_BETA_CONCENTRATION_MAX: 15.0

# ============================================================================
# ENVIRONMENT CONFIGURATION
# ============================================================================

RL_MAX_EPISODE_LENGTH: 50  # Max tokens per generation episode
RL_PROMPT_MIN_LENGTH: 3
RL_PROMPT_MAX_LENGTH: 10
RL_TOP_K: 40  # Top-k sampling during generation

# ============================================================================
# GATING MODULATOR SETTINGS
# ============================================================================

# Base generation parameters (before gating modulation)
RL_BASE_TEMPERATURE: 1.0
RL_BASE_REPETITION_PENALTY: 1.2

# Modulation gate effects on temperature
# Result: base_temp * (min + modulation * (max - min))
RL_MODULATION_TEMP_MIN: 0.3  # At modulation=0 (conservative): temp = 0.3x base
RL_MODULATION_TEMP_MAX: 2.0  # At modulation=1 (exploratory): temp = 2.0x base

# Modulation gate effects on nucleus (top-p) sampling
# Result: top_p_min + modulation * (top_p_max - top_p_min)
RL_MODULATION_TOP_P_MIN: 0.7  # At modulation=0: narrow nucleus
RL_MODULATION_TOP_P_MAX: 1.0  # At modulation=1: full distribution

# Modulation gate effects on repetition penalty
# Result: penalty in range [min, max] based on modulation level
RL_MODULATION_PENALTY_MIN: 1.0  # At modulation=0: no extra penalty
RL_MODULATION_PENALTY_MAX: 3.5  # At modulation=1: 3.5x penalty

# Modulation gate effects on top-k filtering
# Result: top_k_min + modulation * (top_k_max - top_k_min)
RL_MODULATION_TOP_K_MIN: 5    # At modulation=0: sharp sampling (only top 5 tokens)
RL_MODULATION_TOP_K_MAX: 100  # At modulation=1: wide sampling (top 100 tokens)

# ============================================================================
# DYNAMIC GATE TRAINING (EXPERIMENT 3)
# ============================================================================
# When enabled, selectively unfreezes DynamicGate MLPs (~24K params) in the
# frozen TransformerLM. A separate optimizer trains gates to maximize logit
# entropy (diversity) while the PPO agent controls the modulation signal.

RL_UNFREEZE_DYNAMIC_GATES: true
RL_GATE_LR: 1.0e-4           # Learning rate for gate optimizer
RL_GATE_ENTROPY_WEIGHT: 0.1  # Weight for auxiliary logit entropy loss

# ============================================================================
# REWARD FUNCTION WEIGHTS
# ============================================================================

# Multi-component reward: reward = sum(weight_i * component_i)
# Experiment 3: Weighted mode with zero repetition + unfrozen DynamicGates.
# Tests whether unfreezing gate MLPs breaks the monotonic trade-off.
RL_REWARD_PERPLEXITY_WEIGHT: 0.4375  # Lower perplexity = better (quality)
RL_REWARD_DIVERSITY_WEIGHT: 0.1875   # Higher diversity = better (raw logits entropy)
RL_REWARD_SAMPLING_WEIGHT: 0.1875    # Sampling entropy of post-filtered distribution
RL_REWARD_REPETITION_WEIGHT: 0.0     # Zeroed: logit-level penalty handles repetition
RL_REWARD_COHERENCE_WEIGHT: 0.1875   # Higher coherence = better (quality)

# Reward normalization
RL_PERPLEXITY_CLIP: 100.0  # Max perplexity before clipping
RL_ENTROPY_TARGET: 5.0  # Target entropy for diversity (raw logits)
RL_SAMPLING_ENTROPY_TARGET: 2.5  # Target entropy for focus (post-filtered, lower than raw)

# ============================================================================
# EVALUATION AND ABLATION
# ============================================================================

RL_EVAL_EPISODES: 50  # Episodes per policy during ablation
RL_EVAL_INTERVAL: 1000  # Steps between evaluations

# ============================================================================
# LOGGING AND CHECKPOINTS
# ============================================================================

RL_LOG_INTERVAL: 10  # Iterations between console logs
RL_METRICS_INTERVAL: 50  # Iterations between saving metrics
RL_CHECKPOINT_INTERVAL: 100  # Iterations between checkpoints

# TensorBoard tags for RL metrics
TENSORBOARD_TAGS:
  LOSSES: "Loss"
  LEARNING_RATE: "Learning"
  RL_REWARDS: "RL/Rewards"
  RL_POLICY: "RL/Policy"
  RL_GATING: "RL/Gating"
